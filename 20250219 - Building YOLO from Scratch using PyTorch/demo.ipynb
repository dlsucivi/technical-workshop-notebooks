{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building YOLO from Scratch in PyTorch\n",
    "\n",
    "## Workshop Outline\n",
    "0. Dependencies and Libraries\n",
    "1. Going over the YOLO architecture in code\n",
    "2. Preparing the dataset\n",
    "3. Training the model\n",
    "4. Evaluating the model\n",
    "5. Looking at its detections/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Libraries\n",
    "The following is the list of libraries you need to install:\n",
    "- Python (version >= 3.9)\n",
    "- [PyTorch (CUDA version / CPU-Only)](https://pytorch.org/get-started/locally/)\n",
    "    - Torchvision\n",
    "    - Torchaudio\n",
    "- [CUDA](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "    - [Checking GPU compatibility](https://developer.nvidia.com/cuda-gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Architecture Implementation\n",
    "- *Check implementation in model.py and loss.py*\n",
    "\n",
    "### Loading the Model\n",
    "- This initializes the YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import YOLOv1\n",
    "\n",
    "model = YOLOv1(split_size=7, num_boxes=2, num_classes=20).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "    \n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pascal VOC\n",
    "- The dataset is the PascalVOC Dataset which contains 20 classes being:\n",
    "    - aeroplane, bicycle, bird, boat, bottle\n",
    "    - bus, car, cat, chair, cow\n",
    "    - diningtable, dog, horse, motorbike, person\n",
    "    - pottedplant, sheep, sofa, train, tvmonitor\n",
    "\n",
    "- The copy of the dataset can be downloaded here: [PascalVOC_YOLO](https://www.kaggle.com/datasets/734b7bcb7ef13a045cbdd007a3c19874c2586ed0b02b4afc86126e89d00af8d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"dataset/PascalVOC/images/000003.jpg\"\n",
    "\n",
    "image = Image.open(image_path)\n",
    "img_width, img_height = image.size\n",
    "\n",
    "\n",
    "# Class ID, Center_X, Center_Y, Width, Height)\n",
    "bboxes = [\n",
    "    (17, 0.338, 0.4666666666666667, 0.184, 0.10666666666666666),\n",
    "    (8, 0.546, 0.48133333333333334, 0.136, 0.13066666666666665)\n",
    "]\n",
    "\n",
    "CLASS_COLORS = {\n",
    "    8: (0.55, 0.27, 0.07, 1.0),\n",
    "    17: (1.0, 0.41, 0.71, 1.0)\n",
    "}\n",
    "\n",
    "CLASS_MAP = {\n",
    "    8: 'chair',\n",
    "    17: 'sofa'\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "for bbox in bboxes:\n",
    "    class_id, x_center, y_center, width, height = bbox\n",
    "\n",
    "    upper_left_x = (x_center - width / 2) * img_width\n",
    "    upper_left_y = (y_center - height / 2) * img_height\n",
    "    bbox_width = width * img_width\n",
    "    bbox_height = height * img_height\n",
    "    rect = patches.Rectangle(\n",
    "        (upper_left_x, upper_left_y),\n",
    "        bbox_width,\n",
    "        bbox_height,\n",
    "        linewidth=2,\n",
    "        edgecolor=CLASS_COLORS.get(class_id, \"black\"),\n",
    "        facecolor=\"none\"\n",
    "    )\n",
    "\n",
    "    ax.text(\n",
    "        upper_left_x, upper_left_y - 5, f\"{CLASS_MAP[class_id]}\",\n",
    "        color=\"white\",\n",
    "        fontsize=10, weight=\"bold\",\n",
    "        bbox=dict(facecolor=CLASS_COLORS.get(class_id, \"black\"), alpha=0.75)\n",
    "    )\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a dataset object for the PascalVOC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO #\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label, x, y, width, height = [\n",
    "                    float(x) if float(x) != int(float(x)) else int(x)\n",
    "                    for x in label.replace(\"\\n\", \"\").split()\n",
    "                ]\n",
    "\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # Convert To Cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # i,j represents the cell row and cell column\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box,\n",
    "            relative to the cell is done by the following, with\n",
    "            width as the example:\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "            \n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/cell_pixels, simplification leads to the\n",
    "            formulas below.\n",
    "            \"\"\"\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            # If no object already found for specific cell i,j\n",
    "            # Note: This means we restrict to ONE object per cell\n",
    "            if label_matrix[i, j, 20] == 0:\n",
    "                # Set that there exists an object\n",
    "                label_matrix[i, j, 20] = 1\n",
    "\n",
    "                # Box coordinates\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 21:25] = box_coordinates\n",
    "\n",
    "                # Set one hot encoding for class_label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VOCDataset(\n",
    "    \"dataset/PascalVOC/train.csv\",\n",
    "    transform=transform,\n",
    "    img_dir=\"dataset/PascalVOC/images\",\n",
    "    label_dir=\"dataset/PascalVOC/labels\",\n",
    ")\n",
    "\n",
    "test_dataset = VOCDataset(\n",
    "    \"dataset/PascalVOC/test.csv\", \n",
    "    transform=transform, \n",
    "    img_dir=\"dataset/PascalVOC/images\", \n",
    "    label_dir=\"dataset/PascalVOC/labels\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=16,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from loss import YoloLoss\n",
    "from utils import (\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint\n",
    ")\n",
    "\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "loss_fn = YoloLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    loop = tqdm.tqdm(train_loader, leave=True)\n",
    "    avg_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        avg_loss.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Loss: {sum(avg_loss)/len(avg_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "- Loads checkpoint weights if set to `True`. The code is the training loop for the model, alongside the losses it computes the mAP for each epoch on the train set.\n",
    "    - You can download my checkpoint weights here: [weights/checkpoint.pth.tar](https://drive.google.com/file/d/1konF4j8UeFrea-3E3qgc49QWai30b-rs/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "best_map = 0\n",
    "checkpoint = True\n",
    "\n",
    "if checkpoint:\n",
    "      load_checkpoint(torch.load(\"weights/checkpoint.pth.tar\"), model, optimizer)\n",
    "      pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, score_threshold=0.4)\n",
    "      best_map = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        train(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, score_threshold=0.4)\n",
    "        \n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "      # Update save to higher mAP weights\n",
    "      #   if mean_avg_prec > best_map:\n",
    "      #      best_map = mean_avg_prec\n",
    "      #      checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "      #      save_checkpoint(checkpoint, filename=\"weights/checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Visualization\n",
    "- Get mAP scores of trained model on training set and test set and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(torch.load(\"weights/checkpoint.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, score_threshold=0.4)\n",
    "mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "print(f\"Train mAP: {mean_avg_prec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 3\n",
    "count = 0\n",
    "\n",
    "for x, y in train_loader:\n",
    "    x = x.to(device)\n",
    "    for idx in range(8):\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "        bboxes = cellboxes_to_boxes(model(x))\n",
    "        bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "        plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    if count >= num_images:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, score_threshold=0.4)\n",
    "mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "print(f\"Test mAP: {mean_avg_prec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 3\n",
    "count = 0\n",
    "\n",
    "for x, y in test_loader:\n",
    "    x = x.to(device)\n",
    "    for idx in range(8):\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "        bboxes = cellboxes_to_boxes(model(x))\n",
    "        bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "        plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    if count >= num_images:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
