{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2ad6d96",
      "metadata": {
        "id": "c2ad6d96"
      },
      "source": [
        "# Training a YOLO Model Using the NuImages Dataset\n",
        "\n",
        "Welcome! This notebook will guide you through converting the nuImages `v1.0-mini` dataset into the YOLO format required for training an object detection model.\n",
        "\n",
        "We will perform the following steps:\n",
        "1.  **Setup:** Install and import necessary libraries.\n",
        "2.  **Configuration:** Define paths to our input data and output directories.\n",
        "3.  **Helper Functions:** Define the functions that will help us convert formats.\n",
        "4.  **Load & Split:** Load the `v1.0-mini` data and split it into `train` and `val` sets.\n",
        "5.  **Process Data:**\n",
        "    * Copy the images into new `train/images` and `val/images` folders.\n",
        "    * Convert the annotations into YOLO-format `.txt` files and save them in `train/labels` and `val/labels`.\n",
        "6.  **Verification:** Check our new dataset structure.\n",
        "7. **Model Training**: Use our formatted dataset to a YOLO model.\n",
        "8. **Detecting Using a pretrained model**: We will use a pretrained model to show the results of a trained model from nuImages dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfa9f51",
      "metadata": {
        "id": "2bfa9f51"
      },
      "source": [
        "## Setup and Imports\n",
        "*This section outlines the tools, libraries, and methodologies necessary for processing and analyzing the dataset.*\n",
        "\n",
        "Before you begin, make sure you have the required libraries installed. Below includes the list of libraries that we will use to run the notebook.\n",
        "\n",
        "1. **Dataset Manipulation**\n",
        "- `nuimages`: This is library developed by Motional the author behind the nuScenes and nuImages dataset. This library will be used to manipulate and make changes to the dataset. To be able to format it according to the YOLO format.\n",
        "\n",
        "2. **Progress Bars**\n",
        "- `tqdm`: This library is used for progress bars. This would help determine the progress of converting the dataset. Since converting large amount of images would take a long time.\n",
        "\n",
        "3. **Data Processing and Splitting**\n",
        "- `train_test_split`: This is used to determine the split of the dataset. This can be configured by the user but this notebook will use a split of 80% for training and 20% for validation. This is done to show that you can split the dataset.\n",
        "\n",
        "- `numpy`:A core library for numerical operations in Python. It's used by our helper script (utils.py) to perform the mathematical conversions for bounding boxes.\n",
        "\n",
        "4. **Core Python & System Utilities**\n",
        "- `logging`: This is a built-in Python library for emitting status messages and warnings. We use it to get clean, informative output about the script's progress.\n",
        "\n",
        "- `path`: A modern, built-in Python library for handling file system paths. We use its Path object to easily create, join, and manage directories and file paths in a way that works on all operating systems\n",
        "\n",
        "- `shutil`:This is Python's \"shell utilities\" library. We use it for high-level file operations, specifically shutil.copy(), to copy the images from the original dataset into our new YOLO-formatted folders.\n",
        "\n",
        "5. **Model Training**\n",
        "- `ultralytics`: This is the official library for the YOLOv8 model. While we won't use it for the conversion part of this notebook, it is included in the requirements because it will be used in a later step to train our model on the dataset we are creating.\n",
        "\n",
        "6. **Visualization and Verification**\n",
        "\n",
        "- `matplotlib`: This is the most popular plotting library in Python. We will use it to display our images and comparison plots directly inside the Jupyter notebook.\n",
        "\n",
        "- `cv2` (OpenCV): We'll use it to read images from disk and, more importantly, to draw the bounding boxes and labels onto them.\n",
        "\n",
        "- `random`: This is a built-in Python library that we'll use to select a random image from our validation set for our sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8205d8e7",
      "metadata": {
        "id": "8205d8e7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nuimages import NuImages\n",
        "from classes import simplify_nuimage_labels, NuImageSimpleCategory, NuImageSimplerCategory, NuImageSimplestCategory, LabelMappingTypes\n",
        "from utils import PxyXY_to_Nxcycwh\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heUyqorZcg9n",
      "metadata": {
        "id": "heUyqorZcg9n"
      },
      "source": [
        "## Setting up path\n",
        "\n",
        "Here we will setup the different paths needed in splitting the dataset.\n",
        "\n",
        "- `NUIM_ROOT`: This is the directory where the nuImages dataset will be. For this notebook, we will be using `nuImages_mini` which only contains 50 images with labels.\n",
        "\n",
        "- `OUTPUT_ROOT`: This is the directory of the YOLO Formatted nuImages dataset.\n",
        "\n",
        "- `LABEL_MAPPING`: This determines how the classes of the formatted dataset will be categorized. There are 3 ways that the dataset can be formatted. `FAITHFUL` Retains all original classes that came with the nuImages dataset. `SIMPLER` turns the nuImages dataset into 4 classes. And finally we have `SIMPLEST`, Which contains 2 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e0beef",
      "metadata": {
        "id": "28e0beef",
        "outputId": "7524ef17-df2d-4bcd-b116-f52f74ec3c25"
      },
      "outputs": [],
      "source": [
        "NUIM_ROOT = Path(\"nuImagesMini\")\n",
        "\n",
        "OUTPUT_ROOT = Path(\"nuImagesMini_YOLO\")\n",
        "\n",
        "LABEL_MAPPING = \"FAITHFUL\"\n",
        "\n",
        "VAL_SPLIT_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(f\"Input root:  {NUIM_ROOT.resolve()}\")\n",
        "print(f\"Output root: {OUTPUT_ROOT.resolve()}\")\n",
        "print(f\"Label map:   {LABEL_MAPPING}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NuKDYHKfutrp",
      "metadata": {
        "id": "NuKDYHKfutrp"
      },
      "source": [
        "## Declaring Helper Functions\n",
        "\n",
        "- `mkdir_output_dirs`: This is a simple utility function that creates the final directory structure that YOLOv8 expects. It will create `train/images`, `train/labels`, `val/images`, and `val/labels` inside our `OUTPUT_ROOT` directory.\n",
        "\n",
        "- `get_filename_no_suffix`: A small helper that takes a nuImages annotation and finds the original filename of the camera image it belongs to (`n008-2018-08-27-11-48-54-500__CAM_FRONT__1535385057512404`)\n",
        "\n",
        "- `convert_annotation`: This is the most important function. It takes a single nuImages annotation and does three things:\n",
        "\n",
        "  - Finds the original bounding box. For example: `[100, 200, 150, 250]`.\n",
        "  - Converts it to the YOLO format `[0.5, 0.6, 0.02, 0.05]` using the PxyXY_to_Nxcycwh function from our `utils.py` file.\n",
        "  - Maps the complex nuImages category to a simple YOLO class ID using our `classes.py` file.\n",
        "\n",
        "- `append_txt`: This function takes the converted YOLO annotation and writes it as a new line in the correct `.txt` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcb5f6a",
      "metadata": {
        "id": "5dcb5f6a"
      },
      "outputs": [],
      "source": [
        "def mkdir_output_dirs(p: Path):\n",
        "    \"\"\"Creates the train/val output directories for YOLO.\"\"\"\n",
        "    logger.info(f\"Creating output directories at: {p}...\")\n",
        "\n",
        "    # We only need train and val for the mini-dataset\n",
        "    (p / 'train' / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (p / 'train' / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    (p / 'val' / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (p / 'val' / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_filename_no_suffix(annotation, nuim):\n",
        "    \n",
        "    # get token that connects image and annotation\n",
        "    sample_data_token = annotation['sample_data_token']\n",
        "\n",
        "    # retrieves a dictionary containing details about the image\n",
        "    sample_data = nuim.get(\"sample_data\", sample_data_token)\n",
        "    return Path(sample_data['filename']).with_suffix('').name\n",
        "\n",
        "def convert_annotation(annotation, nuim, label_mapping):\n",
        "    \n",
        "    # raw bounding box coordinates\n",
        "    xyXY = annotation['bbox']\n",
        "\n",
        "    # checks if a segmentation mask exists\n",
        "    if annotation['mask'] is None:\n",
        "        return None, None, None\n",
        "    \n",
        "    # extracts the image height and width\n",
        "    height = annotation['mask']['size'][0]\n",
        "    width = annotation['mask']['size'][1]\n",
        "\n",
        "    # convert the coordinates\n",
        "    yolo_bbox_coords = PxyXY_to_Nxcycwh(xyXY, width, height)\n",
        "    if yolo_bbox_coords is None:\n",
        "        return None, None, None\n",
        "\n",
        "    yolo_bbox = list(yolo_bbox_coords)\n",
        "\n",
        "    # look up raw category name and attribute\n",
        "    nu_cat = nuim.get('category', annotation['category_token'])['name']\n",
        "\n",
        "    if annotation['attribute_tokens']:\n",
        "        attribute_token = annotation['attribute_tokens'][0]\n",
        "        attribute = nuim.get('attribute', attribute_token)['name']\n",
        "    else:\n",
        "        attribute = None\n",
        "\n",
        "    yolo_cat = simplify_nuimage_labels(nu_cat, attribute, label_mapping)\n",
        "\n",
        "    # get filename without suffix\n",
        "    filename_no_suffix = get_filename_no_suffix(annotation, nuim)\n",
        "\n",
        "    return yolo_cat, yolo_bbox, filename_no_suffix\n",
        "\n",
        "def append_txt(cat: str, bbox: list, filename_no_suffix: str, set_type: str, output_root: Path, label_mapping: str):\n",
        "\n",
        "    #build the full path to the text file\n",
        "    pa = output_root / set_type / 'labels'\n",
        "    fi = (pa / filename_no_suffix).with_suffix('.txt')\n",
        "\n",
        "    # look up the integer ID from an Enum class based on the strategy\n",
        "    if label_mapping == \"FAITHFUL\":\n",
        "        cat_index = NuImageSimpleCategory[cat].value\n",
        "    elif label_mapping == \"SIMPLER\":\n",
        "        cat_index = NuImageSimplerCategory[cat].value\n",
        "    elif label_mapping == \"SIMPLEST\":\n",
        "        cat_index = NuImageSimplestCategory[cat].value\n",
        "    else:\n",
        "        logger.error(f\"Unknown value for label_mapping: {label_mapping}. Exiting.\")\n",
        "        return\n",
        "    \n",
        "    # unpack bbox\n",
        "    xc, yc, w, h = bbox\n",
        "\n",
        "    # write to file\n",
        "    with open(fi, 'a') as f:\n",
        "        f.write(f\"{cat_index} {xc} {yc} {w} {h}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n2y2PtQ7VzTW",
      "metadata": {
        "id": "n2y2PtQ7VzTW"
      },
      "source": [
        "## Load & Split the Data\n",
        "\n",
        "Now that we have our helper functions, it's time to load the v1.0-mini dataset and prepare it for processing. This cell accomplishes four key steps:\n",
        "\n",
        "1. **Create Directories**: It first calls `mkdir_output_dirs` to create the empty `train/` and `val/` folders inside your `OUTPUT_ROOT`.\n",
        "\n",
        "2. **Load nuImages Data**: It uses the `NuImages` class to load the `v1.0-mini` dataset. We pass `lazy=True` for efficiency, so it only loads data from the JSON files as we need it, rather than all at once.\n",
        "\n",
        "3. **Split the Dataset**: It takes the list of all samples from the mini-dataset and uses `train_test_split` to divide them into a training set (80% of the data) and a validation set (20% of the data).\n",
        "\n",
        "4. **Create Splits Dictionary**: Finally, it creates a simple dictionary named `splits`. This just makes it easier in the following steps to loop over our `train_samples` and `val_samples` lists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2050241",
      "metadata": {
        "id": "c2050241",
        "outputId": "55489add-1701-40b9-8d26-7ecaa0a38fe0"
      },
      "outputs": [],
      "source": [
        "# create output directories\n",
        "mkdir_output_dirs(OUTPUT_ROOT)\n",
        "\n",
        "# load the  dataset\n",
        "print(f\"Loading nuImages v1.0-mini from {NUIM_ROOT.resolve()}\")\n",
        "nuim = NuImages(dataroot=NUIM_ROOT.resolve(), version=\"v1.0-mini\", verbose=False, lazy=True)\n",
        "logger.info(f\"Successfully loaded {len(nuim.sample)} samples from v1.0-mini.\")\n",
        "\n",
        "# split the samples into train val sets\n",
        "train_samples, val_samples = train_test_split(\n",
        "    nuim.sample,\n",
        "    test_size=VAL_SPLIT_SIZE,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "logger.info(f\"Splitting into {len(train_samples)} train and {len(val_samples)} val samples.\")\n",
        "\n",
        "# create dictionary to iterate over\n",
        "splits = {\n",
        "    \"train\": train_samples,\n",
        "    \"val\": val_samples\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fOXxZVZPZxIw",
      "metadata": {
        "id": "fOXxZVZPZxIw"
      },
      "source": [
        "## Process and Copy Images\n",
        "\n",
        "With our data split into `train` and `val` sets, we can now populate our new YOLO directories. This first processing step focuses only on the images.\n",
        "\n",
        "This code block loops through our `splits` dictionary (first for \"train\", then for \"val\"). For each sample in each set, it performs these steps:\n",
        "\n",
        "1. **Find the Image**: It gets the record for the main \"key camera\" image.\n",
        "\n",
        "2. **Get Original Path**: It reads the original file path from the nuImages metadata\n",
        "\n",
        "3. **Get Destination Path**: It builds the new path where the image should go\n",
        "\n",
        "4. **Copy File**: Finally, it uses `shutil.copy()` to copy the image from its original location to our new `train/images` or `val/images` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a718400",
      "metadata": {
        "id": "5a718400",
        "outputId": "118d5bc5-a945-45ad-e6c1-48a42e02ba76"
      },
      "outputs": [],
      "source": [
        "for set_type, samples in splits.items():\n",
        "    logger.info(f\"processing {set_type} images\")\n",
        "\n",
        "    destination_dir = OUTPUT_ROOT / set_type / 'images'\n",
        "\n",
        "    for sample in tqdm(samples, desc=f\"copying {set_type} images\"):\n",
        "        # get sample_data token for the key camera\n",
        "        sample_data_token = sample['key_camera_token']\n",
        "\n",
        "        # get sample_data record\n",
        "        sample_data = nuim.get(\"sample_data\", sample_data_token)\n",
        "\n",
        "        # original image path\n",
        "        origin_jpg_path = NUIM_ROOT / sample_data['filename']\n",
        "\n",
        "        # destination path\n",
        "        destination_jpg_path = destination_dir / origin_jpg_path.name\n",
        "\n",
        "        # copy\n",
        "        if not origin_jpg_path.exists():\n",
        "            logger.warning(f\"Image not found at {origin_jpg_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        shutil.copy(origin_jpg_path, destination_jpg_path)\n",
        "\n",
        "print(\"\\nImage copying complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mEs6OwKLeDSK",
      "metadata": {
        "id": "mEs6OwKLeDSK"
      },
      "source": [
        "## Process and Write Annotations\n",
        "\n",
        "With our images copied, we now need to create the matching `.txt` label files.\n",
        "\n",
        "1. **Create a Lookup Map**: First, it creates a dictionary called `token_to_split_map`. This is a very fast way to check which split each image token belongs to. We do this before looping for efficiency.\n",
        "\n",
        "2. **Iterate All Annotations**: The code then loops through `nuim.object_ann`, which is the master list of every single annotation in the `v1.0-mini` dataset.\n",
        "\n",
        "3. **Check & Skip**: For each annotation, it checks if its `sample_data_token` (the image it belongs to) is in our `token_to_split_map`. If it's not, it means the annotation is for an image we didn't include in our splits, so we simply ignore it and continue.\n",
        "\n",
        "4. **Convert & Write**: If the annotation is in our `map`, we know it's one we need.\n",
        "    * It calls `convert_annotation()` to get the YOLO-formatted bounding box and class ID.\n",
        "    * It then calls `append_txt()` to write that information as a new line in the correct `.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0b5152",
      "metadata": {
        "id": "3c0b5152",
        "outputId": "a3d219a9-02e7-48a5-8494-e57cdb55aa28"
      },
      "outputs": [],
      "source": [
        "# create a lookup map to know which split an annotation belongs to.\n",
        "# map the sample_data_token (which images are annotated) to a split (\"train\" or \"val\").\n",
        "token_to_split_map = {}\n",
        "for sample in train_samples:\n",
        "    token_to_split_map[sample['key_camera_token']] = \"train\"\n",
        "for sample in val_samples:\n",
        "    token_to_split_map[sample['key_camera_token']] = \"val\"\n",
        "\n",
        "logger.info(f\"Converting and writing {len(nuim.object_ann)} total annotations\")\n",
        "\n",
        "# iterate through all object annotations in the mini-dataset\n",
        "for annotation in tqdm(nuim.object_ann, desc=\"Converting annotations\"):\n",
        "\n",
        "    # find out which split this annotation's image belongs to\n",
        "    sample_data_token = annotation['sample_data_token']\n",
        "    if sample_data_token not in token_to_split_map:\n",
        "        continue\n",
        "\n",
        "    set_type = token_to_split_map[sample_data_token]\n",
        "\n",
        "    # convert annotation\n",
        "    cat, bbox, filename_no_suffix = convert_annotation(annotation, nuim, LABEL_MAPPING)\n",
        "\n",
        "    if cat is None:\n",
        "        continue\n",
        "\n",
        "    # Write the annotation line to the correct file\n",
        "    append_txt(cat, bbox, filename_no_suffix, set_type, OUTPUT_ROOT, LABEL_MAPPING)\n",
        "\n",
        "print(\"\\nAnnotation conversion complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56bdbae",
      "metadata": {},
      "source": [
        "## Sanity Check: Visualizing the Conversion (Side-by-Side)\n",
        "\n",
        "After all that work, how do we know if our conversion was successful? The best way is to see it.\n",
        "\n",
        "We will write a script to:\n",
        "1.  Pick a random image from our new validation set.\n",
        "2.  Load the original image.\n",
        "3.  Create two copies:\n",
        "    * **Image 1:** We will draw the **original nuImages annotations** on it in **BLUE**.\n",
        "    * **Image 2:** We will load our **new YOLO-format .txt file** and draw those annotations on it in **RED**.\n",
        "4.  Display them side-by-side.\n",
        "\n",
        "If our conversion worked, the red boxes on the right should perfectly match the blue boxes on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e105d75",
      "metadata": {
        "id": "9e105d75",
        "outputId": "b2022247-88bf-4141-80de-c7fdd224c08f"
      },
      "outputs": [],
      "source": [
        "logger.info(\"Running visual sanity check...\")\n",
        "\n",
        "if LABEL_MAPPING == \"FAITHFUL\":\n",
        "    yolo_class_names = {cat.value: cat.name for cat in NuImageSimpleCategory}\n",
        "else:\n",
        "    yolo_class_names = {0: \"class_0\", 1: \"class_1\"}\n",
        "\n",
        "random_sample = random.choice(val_samples)\n",
        "sample_data_token = random_sample['key_camera_token']\n",
        "sample_data = nuim.get(\"sample_data\", sample_data_token)\n",
        "\n",
        "yolo_img_path = str(OUTPUT_ROOT / \"val\" / \"images\" / Path(sample_data['filename']).name)\n",
        "yolo_label_path = str(OUTPUT_ROOT / \"val\" / \"labels\" / Path(sample_data['filename']).with_suffix('.txt').name)\n",
        "\n",
        "\n",
        "img = cv2.imread(yolo_img_path)\n",
        "if img is None:\n",
        "    logger.error(f\"Could not read image at {yolo_img_path}\")\n",
        "else:\n",
        "    img_height, img_width, _ = img.shape\n",
        "    logger.info(f\"Checking image: {Path(yolo_img_path).name}\")\n",
        "    \n",
        "    img_original = img.copy()\n",
        "    img_yolo = img.copy()\n",
        "\n",
        "    original_anns = [ann for ann in nuim.object_ann if ann['sample_data_token'] == sample_data_token]\n",
        "    \n",
        "    for ann in original_anns:\n",
        "\n",
        "        x1, y1, x2, y2 = [int(c) for c in ann['bbox']]\n",
        "\n",
        "        cat_token = ann['category_token']\n",
        "        cat_name = nuim.get('category', cat_token)['name'].split('.')[-1]\n",
        "\n",
        "        cv2.rectangle(img_original, (x1, y1), (x2, y2), (255, 0, 0), 2) # BLUE\n",
        "        cv2.putText(img_original, cat_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "    try:\n",
        "        with open(yolo_label_path, 'r') as f:\n",
        "            for line in f:\n",
        "\n",
        "                class_id, x_center, y_center, w_norm, h_norm = [float(x) for x in line.strip().split()]\n",
        "                class_id = int(class_id)\n",
        " \n",
        "                box_w = w_norm * img_width\n",
        "                box_h = h_norm * img_height\n",
        "                x_min = int((x_center * img_width) - (box_w / 2))\n",
        "                y_min = int((y_center * img_height) - (box_h / 2))\n",
        "                x_max = int(x_min + box_w)\n",
        "                y_max = int(y_min + box_h)\n",
        "\n",
        "                label_name = yolo_class_names.get(class_id, \"UNKNOWN\")\n",
        "\n",
        "                cv2.rectangle(img_yolo, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2) # RED\n",
        "                cv2.putText(img_yolo, label_name, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logger.warning(f\"No YOLO label file found at: {yolo_label_path}\")\n",
        "        \n",
        "    logger.info(\"Displaying comparison: Left (Original), Right (YOLO)\")\n",
        "    \n",
        "    img_original_rgb = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
        "    img_yolo_rgb = cv2.cvtColor(img_yolo, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12))\n",
        "\n",
        "    ax1.imshow(img_original_rgb)\n",
        "    ax1.set_title(\"Original nuImages Annotations (BLUE)\", fontsize=16)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    ax2.imshow(img_yolo_rgb)\n",
        "    ax2.set_title(\"Converted YOLO Annotations (RED)\", fontsize=16)\n",
        "    ax2.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e1aaca",
      "metadata": {},
      "source": [
        "## Creating the yaml file\n",
        "\n",
        " **Auto-generate the `data.yaml` file:** We will write a script to create the dataset configuration file that YOLOv8 needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f046d21",
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(\"Creating data.yaml file...\")\n",
        "\n",
        "# 1. Define the path where the YAML file will be saved\n",
        "yaml_path = OUTPUT_ROOT / \"nuimages_mini.yaml\"\n",
        "\n",
        "# 2. Get the class names based on the mapping you chose\n",
        "class_names = []\n",
        "if LABEL_MAPPING == \"FAITHFUL\":\n",
        "    class_names = [cat.name for cat in NuImageSimpleCategory]\n",
        "elif LABEL_MAPPING == \"SIMPLER\":\n",
        "    class_names = [cat.name for cat in NuImageSimplerCategory]\n",
        "elif LABEL_MAPPING == \"SIMPLEST\":\n",
        "    class_names = [cat.name for cat in NuImageSimplestCategory]\n",
        "\n",
        "if not class_names:\n",
        "    logger.error(\"Could not determine class names. Make sure LABEL_MAPPING is set.\")\n",
        "else:\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # We will get the full, absolute path to our output directory\n",
        "    # .resolve() converts \"nuimagesMini_YOLO\" to \"D:\\Academic Workshop\\nuimagesMini_YOLO\"\n",
        "    absolute_path = OUTPUT_ROOT.resolve()\n",
        "    \n",
        "    # Create the YAML content using the absolute path\n",
        "    yaml_content = f\"\"\"\n",
        "path: {absolute_path}\n",
        "train: train/images\n",
        "val: val/images\n",
        "\n",
        "# Class names\n",
        "names:\n",
        "\"\"\"\n",
        "    # Add all class names to the file\n",
        "    for i, name in enumerate(class_names):\n",
        "        yaml_content += f\"  {i}: {name}\\n\"\n",
        "\n",
        "    # 3. Write the content to the file\n",
        "    try:\n",
        "        with open(yaml_path, 'w') as f:\n",
        "            f.write(yaml_content)\n",
        "        \n",
        "        logger.info(f\"Successfully created data configuration file at: {yaml_path}\")\n",
        "        print(f\"\\n--- Contents of {yaml_path} ---\")\n",
        "        print(yaml_content)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to write YAML file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ccd558",
      "metadata": {},
      "source": [
        "### Finally we have all the requirements to start training and predicting with the model. Let us now move on to the `Training` portion of the workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbee945d",
      "metadata": {},
      "source": [
        "# References\n",
        "\n",
        "- `classes.py` and `utils.py` is referenced from this repository: https://github.com/tensorturtle/yolov8-on-nuimages\n",
        "- `nuImages devkit` can be found in this repository: https://github.com/nutonomy/nuscenes-devkit"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2bfa9f51",
        "heUyqorZcg9n",
        "NuKDYHKfutrp",
        "n2y2PtQ7VzTW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nuscenes_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
