{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daae8053",
   "metadata": {},
   "source": [
    "# Training an Object Segmentation Workshop\n",
    "### By: Aaron Gabrielle C. Dichoso\n",
    "### From: DLSU - Center of Imaging and Visual Innovations (CIVI)\n",
    "May 27, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d9d20",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61500afa",
   "metadata": {},
   "source": [
    "## 2. Notebook configurations\n",
    "Batch Size: The Number of Images passed to the model during training in one forward pass\n",
    "\n",
    "Classes: Subset of 12 objects + 1 background class from the standard MSCOCO classes\n",
    "\n",
    "[\"dog\", \"cat\", \"person\", \"chair\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"cup\", \"fork\", \"knife\", \"spoon\"]\n",
    "\n",
    "Epochs: Number of Iterations that the training images will be passed to the model.\n",
    "\n",
    "Learning Rate: Affects the strength of adjustments applied to the model during training.\n",
    "\n",
    "Device: Use CUDA if available, else use the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ed884",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 12 + 1\n",
    "NUM_EPOCHS = 45\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "COCO_PATH = './dataset/'\n",
    "TRAIN_IMG_DIR = os.path.join(COCO_PATH, 'coco_sample/train')\n",
    "TEST_IMG_DIR = os.path.join(COCO_PATH, 'coco_sample/test')\n",
    "VAL_IMG_DIR = os.path.join(COCO_PATH, 'coco_sample/val')\n",
    "TRAIN_ANN_FILE = os.path.join(COCO_PATH, 'coco_sample/train.json')\n",
    "TEST_ANN_FILE = os.path.join(COCO_PATH, 'coco_sample/test.json')\n",
    "VAL_ANN_FILE = os.path.join(COCO_PATH, 'coco_sample/val.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cb5a9",
   "metadata": {},
   "source": [
    "## 3. Dataset Loader\n",
    "\n",
    "A dataset loader is utilized in this notebook to allow modifications to the original MSCOCO dataset:\n",
    "\n",
    "1. Instead of the 91 classes in MSCOCO, only 13 classes are used\n",
    "2. Allows you to apply transformations to images before loading the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd413a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoSegmentation(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annFile):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902367d",
   "metadata": {},
   "source": [
    "## 4. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = None\n",
    "test_dataset = None\n",
    "val_dataset = None\n",
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37ffc6",
   "metadata": {},
   "source": [
    "## 5. FCN Model Configurations\n",
    "\n",
    "Get the FCN model from pytorch, and modify it to use the custom number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba020722",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fcn_resnet50(pretrained=False, num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92876da",
   "metadata": {},
   "source": [
    "Criterion: Measure of Model Performance used during training.\n",
    "\n",
    "Optimizer: Method used to determine the optimal weights during training.\n",
    "\n",
    "Scheduler: Decays the Learning Rate over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dae939",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = None\n",
    "optimizer = None\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b00d2",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "General Flow is as follows:\n",
    "1. Set the model into training mode\n",
    "2. For each epoch, do the following:\n",
    "    a. Load the Images and Masks to the Device\n",
    "    b. Zero out existing gradients\n",
    "    c. Perform a Forward Pass\n",
    "    d. Compute the Prediction Performance / Loss\n",
    "    e. Perform a backward pass and update weights (Backpropagation)\n",
    "    f. save the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")  # Create a directory to save checkpoints\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):  # For each epoch\n",
    "\n",
    "        for images, masks in progress_bar:\n",
    "            images, masks = images.to(device), masks.to(device)  # Load Images and Masks to Device\n",
    "\n",
    "            continue\n",
    "\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b120d49",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013cb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(preds, labels):\n",
    "    pass\n",
    "\n",
    "def mean_iou(preds, labels, num_classes):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    accs, ious = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            continue\n",
    "\n",
    "    print(f\"Pixel Accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Mean IoU: {np.mean(ious):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c1ab6",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as FNN\n",
    "\n",
    "def visualize_random_samples(model, dataset, class_names, num_samples=5):\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        with torch.no_grad():\n",
    "            output = model(image.unsqueeze(0).to(DEVICE))['out']  # shape: (1, C, H, W)\n",
    "            probs = FNN.softmax(output, dim=1).squeeze(0)          # shape: (C, H, W)\n",
    "            pred = torch.argmax(probs, dim=0).cpu().numpy()      # shape: (H, W)\n",
    "            # Calculate average confidence for each predicted class in the mask\n",
    "            unique_labels = np.unique(pred)\n",
    "            confs = {}\n",
    "            for label in unique_labels:\n",
    "                mask_pixels = (pred == label)\n",
    "                # Average confidence for that class at predicted pixels\n",
    "                conf = probs[label][mask_pixels].mean().item()\n",
    "                confs[label] = conf\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image.permute(1, 2, 0))\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred)\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Sample index: {idx}\")\n",
    "        print(\"Class labels in prediction and average confidence:\")\n",
    "        for label in unique_labels:\n",
    "            name = class_names.get(label, \"N/A\")\n",
    "            confidence = confs[label]\n",
    "            print(f\"Label {label}: {name} - Confidence: {confidence:.3f}\")\n",
    "        print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_samples(model, test_dataset, test_dataset.class_names, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
